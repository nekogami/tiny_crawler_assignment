#!/usr/bin/env ruby

require 'optparse'
require 'fileutils'
require "open-uri"
require "uri"
require "pry-byebug"
module PageCrawler
  SAVE_FOLDER = Dir.home + "/fetch/"
  HTTP_TIMEOUT = 5
  TIME_FORMAT = "%a %b %Y %H:%M %Z"

  class << self
    #
    # Simple command line parser
    #
    # @param [Array<String>] args List of all arguments
    #
    # @return [Hash<Symbol, Boolean] return a hash of options
    #
    def parse_options(args)
      options = {}

      option_parser = OptionParser.new do | opts |

        opts.on("--metadata", "Print metadata instead") do | opt |
          options[:metadata] = opt
        end
      end

      option_parser.parse!(ARGV)

      return options
    end

    #
    # Print on stdout metadata of the saved html file
    #
    # @param [String] file_path Path to an existing html file created from #save_page_to_local
    #
    def print_page_metadata(file_path:)
      file_stats = File.stat(file_path)
      html = File.read(file_path)

      stats = <<~FSTATS
        site: #{File.basename(file_path, ".html")}
        numlink: #{html.scan(/<a [^>]*>/).count}
        images: #{html.scan(/<img [^>]*>/).count}
        last_fetch: #{file_stats.ctime.utc.strftime(TIME_FORMAT)}\n
      FSTATS

      puts stats
    end

    #
    # Attempt to save page to local disk,
    #
    # @param [<URI>] uri URI instance
    # @param [<String>] dir_path Path to the save folder (must already exists)
    #
    # @return [String] File path to the saved file
    #
    def save_page_to_local(uri:, dir_path: SAVE_FOLDER)
      file_path = "#{dir_path}#{uri.host}.html"

      File.open(file_path, "w") { | f | f.write(uri.open(read_timeout: HTTP_TIMEOUT).read) }

      file_path
    end

    #
    # Save html to files for each url given, and apply options as necessary
    #
    # @param [Array<String>] urls array of strings corresponding to all passed urls
    # @param [Hash<Symbol, Boolean>] options List of options and their values { metadata: true/false }
    # @param [Boolean] raise_exception flag to raise exception or not when during the page fetch
    #
    def fetch_pages_data(urls:, options:, raise_exception: true)
      # Ensure save folder path exists
      FileUtils.mkdir_p(SAVE_FOLDER)

      begin
        urls.each do | url |
          uri = URI.parse(url)

          file_path = save_page_to_local(uri: uri)
          print_page_metadata(file_path: file_path) if !!options[:metadata]
        end
      rescue => e
        puts e
        raise e if raise_exception
      end
    end

    #
    # Entry point for that module
    #
    # @param [Array<String>] args array of strings corresponding to all passed arguments
    #
    def fetch_pages(args)
      options = parse_options(args)

      fetch_pages_data(urls: ARGV, options: options)
    end
  end
end

PageCrawler.fetch_pages(ARGV)
